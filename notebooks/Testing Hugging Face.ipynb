{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install and import necessary libraries\n",
        "!pip install accelerate datasets emoji pandas sklearn torch torchvision transformers xformers\n",
        "from datasets import load_dataset\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "id": "Q-AcLPOpXA2n",
        "outputId": "4634a386-87d5-4ff0-9a5a-b27d78361747"
      },
      "id": "Q-AcLPOpXA2n",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "Collecting datasets\n",
            "  Using cached datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "Collecting emoji\n",
            "  Using cached emoji-2.5.1.tar.gz (356 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post5.tar.gz (3.7 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-431d6c39221e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Install and import necessary libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install accelerate datasets emoji pandas sklearn torch torchvision transformers xformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "31tuiV_QF3YR"
      },
      "id": "31tuiV_QF3YR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process data\n",
        "def load_and_process_data():\n",
        "    if os.path.exists(\"bad_reviews.csv\"):\n",
        "       bad_reviews = pd.read_csv(\"bad_reviews.csv\")\n",
        "    else:\n",
        "      dataset = load_dataset(\"amazon_us_reviews\", \"Digital_Video_Games_v1_00\")\n",
        "      data_df = pd.DataFrame.from_dict(dataset['train'])\n",
        "      bad_reviews = data_df.loc[(data_df[\"star_rating\"] == 1) & (data_df['review_body'].str.len() < 128)].copy()\n",
        "      if bad_reviews.empty:\n",
        "          print(\"No reviews meet the criteria. Please check your filtering process.\")\n",
        "          return None\n",
        "      bad_reviews.to_csv(\"bad_reviews.csv\")\n",
        "      return bad_reviews"
      ],
      "metadata": {
        "id": "ugsVxqL3F4zp"
      },
      "id": "ugsVxqL3F4zp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis\n",
        "def sentiment_analysis(bad_reviews):\n",
        "    # Check if cached results exist\n",
        "    if os.path.exists(\"sentiment_analysis_results.csv\"):\n",
        "        # Load cached results\n",
        "        bad_reviews = pd.read_csv(\"sentiment_analysis_results.csv\")\n",
        "    else:\n",
        "        # Perform sentiment analysis\n",
        "        model_name = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
        "        sentiment_pipeline = pipeline(\"text-classification\", model=model_name, device=device)\n",
        "\n",
        "        sentiments = sentiment_pipeline(bad_reviews['review_body'].tolist())\n",
        "\n",
        "        sentiments_df = pd.DataFrame(sentiments)\n",
        "        bad_reviews.reset_index(inplace=True, drop=True)\n",
        "        sentiments_df.reset_index(inplace=True, drop=True)\n",
        "        # Add sentiment label and score to bad_reviews DataFrame\n",
        "        bad_reviews['sentiment_label'] = sentiments_df['label']\n",
        "        bad_reviews['sentiment_score'] = sentiments_df['score']\n",
        "\n",
        "        # Filter bad_reviews based on sentiment label and score\n",
        "        bad_reviews = bad_reviews.loc[(bad_reviews['sentiment_label'].isin(['NEGATIVE', 'NEU'])) & (bad_reviews['sentiment_score'] > 0.7)]\n",
        "\n",
        "        # Cache results\n",
        "        bad_reviews.to_csv(\"sentiment_analysis_results.csv\")\n",
        "\n",
        "        return bad_reviews"
      ],
      "metadata": {
        "id": "0gO9Tl5eJqwd"
      },
      "id": "0gO9Tl5eJqwd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and load data\n",
        "def save_and_load_data(bad_reviews):\n",
        "    bad_reviews.to_csv(\"cleaner_data.csv\")\n",
        "    cleaned_data = pd.read_csv(\"cleaner_data.csv\")\n",
        "    return cleaned_data"
      ],
      "metadata": {
        "id": "jZDbsVs9F7qp"
      },
      "id": "jZDbsVs9F7qp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_train_model(cleaned_data):\n",
        "    if cleaned_data.empty:\n",
        "        print(\"The cleaned data is empty. Please check your data cleaning process.\")\n",
        "        return None\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "        # Check if trained model file exists\n",
        "        if os.path.exists(\"trained_model.pt\"):\n",
        "            # Load trained model from file\n",
        "            model = AutoModelForCausalLM.from_pretrained(\"trained_model.pt\")\n",
        "        else:\n",
        "            # Train model\n",
        "            model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "            # Split data into training and test sets\n",
        "            train, test = train_test_split(cleaned_data.review_body, test_size = 0.15)\n",
        "            train.to_csv(\"train.csv\")\n",
        "            test.to_csv(\"test.csv\")\n",
        "\n",
        "            # Load datasets\n",
        "            train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"train.csv\", block_size=128)\n",
        "            test_dataset = TextDataset(tokenizer=tokenizer, file_path=\"test.csv\", block_size=128)\n",
        "            data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "            # Define training arguments\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=\"logs\",\n",
        "                overwrite_output_dir=True,\n",
        "                num_train_epochs=3,\n",
        "                per_device_train_batch_size=32,\n",
        "                per_device_eval_batch_size=64,\n",
        "                eval_steps = 400,\n",
        "                save_steps=800,\n",
        "                warmup_steps=500,\n",
        "                prediction_loss_only=True,\n",
        "            )\n",
        "\n",
        "            # Train the model\n",
        "            trainer = Trainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                data_collator=data_collator,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=test_dataset,\n",
        "            )\n",
        "            trainer.train()\n",
        "            trainer.save_model(\"trained_model.pt\")\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "CReYJe_DF8s5"
      },
      "id": "CReYJe_DF8s5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "def generate_text(model):\n",
        "    if model is not None:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "        generator = pipeline('text-generation', model=model.to(device), tokenizer=tokenizer, device=device.index)\n",
        "\n",
        "        prompt1 = \"I HATE MY JOB\"\n",
        "        prompt2 = \"Nobody understands me\"\n",
        "\n",
        "        print(generator(prompt1, max_length=150, num_return_sequences=3))\n",
        "        print(generator(prompt2, max_length=150))"
      ],
      "metadata": {
        "id": "1PKsNxUzF92J"
      },
      "id": "1PKsNxUzF92J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_reviews = load_and_process_data()\n",
        "bad_reviews = sentiment_analysis(bad_reviews)\n",
        "if bad_reviews is not None:\n",
        "    cleaned_data = save_and_load_data(bad_reviews)\n",
        "    model = load_and_train_model(cleaned_data)\n",
        "    generate_text(model)"
      ],
      "metadata": {
        "id": "0aOw4PjiHOze"
      },
      "id": "0aOw4PjiHOze",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}